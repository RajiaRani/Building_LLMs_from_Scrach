{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b51d88d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We implemented a simple tokenization scheme in the previous sections for illustration\n",
    "purposes. \n",
    "\n",
    "This section covers a more sophisticated tokenization scheme based on a concept\n",
    "called byte pair encoding (BPE). \n",
    "\n",
    "The BPE tokenizer covered in this section was used to train\n",
    "LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ae3362",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Since implementing BPE can be relatively complicated, we will use an existing Python\n",
    "open-source library called tiktoken (https://github.com/openai/tiktoken). \n",
    "\n",
    "This library implements\n",
    "the BPE algorithm very efficiently based on source code in Rust.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71c145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2025.9.18)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9088e826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import  tiktoken\n",
    "print(\"tiktoken version\" , importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adbad58",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbe309c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented previously via\n",
    "an encode method:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebeb4bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc23be",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The code above prints the following token IDs:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e738be0d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We can then convert the token IDs back into text using the decode method, similar to our\n",
    "SimpleTokenizerV2 earlier:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61741cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5658542",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can make two noteworthy observations based on the token IDs and decoded text\n",
    "above. \n",
    "\n",
    "First, the <|endoftext|> token is assigned a relatively large token ID, namely,\n",
    "50256. \n",
    "\n",
    "In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3,\n",
    "and the original model used in ChatGPT, has a total vocabulary size of 50,257, with\n",
    "<|endoftext|> being assigned the largest token ID.\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89206b54",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Second, the BPE tokenizer above encodes and decodes unknown words, such as\n",
    "\"someunknownPlace\" correctly. \n",
    "\n",
    "The BPE tokenizer can handle any unknown word. How does\n",
    "it achieve this without using <|unk|> tokens?\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea9a5d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary\n",
    "into smaller subword units or even individual characters.\n",
    "\n",
    "The enables it to handle out-ofvocabulary words. \n",
    "\n",
    "So, thanks to the BPE algorithm, if the tokenizer encounters an\n",
    "unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or\n",
    "characters\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea818b",
   "metadata": {},
   "source": [
    "**Let us take another simple example to illustrate how the BPE tokenizer deals with unknown tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81e4b22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
