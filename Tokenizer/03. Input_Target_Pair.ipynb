{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7cab49",
   "metadata": {},
   "source": [
    "### DATA SAMPLING WITH SLIDING WINDOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf61ea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e7aedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2025.9.18)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c212e163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import  tiktoken\n",
    "print(\"tiktoken version\" , importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b22e720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59bf5f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To get started, we will first tokenize the whole The Verdict short story we worked with\n",
    "earlier using the BPE tokenizer introduced in the previous section:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab8d688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5409dd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Executing the code above will return 5145, the total number of tokens in the training set,\n",
    "after applying the BPE tokenizer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef888b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it\n",
    "results in a slightly more interesting text passage in the next steps:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c3e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the first 50 tokens for more clarity\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65442198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5095\n"
     ]
    }
   ],
   "source": [
    "print(len(enc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27fb6c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens\n",
    "and y contains the targets, which are the inputs shifted by 1:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84d13b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The context size determines how many tokens are included in the input\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61bc63bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [290, 4920, 2241, 287]\n",
      "Y: [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #length of the input\n",
    "# The context_size of 4 means that the model is trained to look at a sequence of 4 words(or tokens)\n",
    "# To predict the next word(token) in the sequence\n",
    "# The input X is the first 4 tokens [1,2,3,4] and the target(output) Y is the next tokens [2,3,4,5]\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"X: {x}\")\n",
    "print(f\"Y: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb81ba",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Processing the inputs along with the targets, which are the inputs shifted by one position,\n",
    "we can then create the next-word prediction tasks as\n",
    "follows:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4d53d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -------> 4920\n",
      "[290, 4920] -------> 2241\n",
      "[290, 4920, 2241] -------> 287\n",
      "[290, 4920, 2241, 287] -------> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"------->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4fcde",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token\n",
    "ID on the right side of the arrow represents the target token ID that the LLM is supposed to\n",
    "predict.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7653991e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For illustration purposes, let's repeat the previous code but convert the token IDs into\n",
    "text:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db76d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ------>  established\n",
      " and established ------>  himself\n",
      " and established himself ------>  in\n",
      " and established himself in ------>  a\n"
     ]
    }
   ],
   "source": [
    "for i in range (1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"------>\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7d4ec7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We've now created the input-target pairs that we can turn into use for the LLM training in\n",
    "upcoming chapters.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ed519",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that\n",
    "iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which\n",
    "can be thought of as multidimensional arrays.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be8320",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In particular, we are interested in returning two tensors: an input tensor containing the\n",
    "text that the LLM sees and a target tensor that includes the targets for the LLM to predict,\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903819c",
   "metadata": {},
   "source": [
    "### IMPLEMENTING A DATA LOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4ed94c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and\n",
    "DataLoader classes.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d22a177",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Tokenize the entire text\n",
    "    \n",
    "Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "\n",
    "Step 3: Return the total number of rows in the dataset\n",
    "\n",
    "Step 4: Return a single row from the dataset\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad46a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenizer the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
